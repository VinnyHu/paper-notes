# Is Word Segmentation Necessary for Deep Learning of Chinese Representations?
* 这篇文章是在知乎上面看到的，paper的作者之一李纪为在知乎上面写过关于nlp学习的文章，个人感觉写的极好，所以就收藏+关注了，然后这是前段时间看到他发的文章，在ACL2019上面。

* 这篇文章考察的是基于深度学习的中文nlp任务中分词的必要性，文章在四个任务上面测试——语言模型、机器翻译、文不匹配、文本分类，最后得到的结果是直接基于字符的效果好于基于分词的。

* 差不多在paper中主要讨论了分词在四大nlp任务上的表现，其实都是弱于不分词的，paper先说实验配置再摆结果，最后又探讨了作者认为的分词导致的效果不好——数据稀疏性、OOV、参数过多导致过拟合等问题。

  后续可以看一些文章，研究下分词对于任务到底有没有好的贡献，如果有是哪一方面的贡献呢？

OOV：

Zipf's定律：

* why？

  为什么单词的稀疏性会导致过拟合